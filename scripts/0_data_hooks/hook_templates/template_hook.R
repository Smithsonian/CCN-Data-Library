# Coastal Carbon Research Coordination Network
# Template data hook for the CCRCN Data Library
# Contact: klingesd@si.edu
#          lonnemanm@si.edu

print("Add study citation(s) here in the format you so please, or the format
      given on the DOI(s), so long as you include the DOI(s) as well")

## 0. Workspace prep ###################

# NOTE: 

# Not all of these packages will be helpful for every hook:
# - if you don't download the data, you can remove the rvest and RCurl
# - if you don't need to import excel spreadsheets to R (as in, your data is in
#   .csv or tab-delimited formats). you can remove readxl
# - if you won't be working with date or time series data, you can remove lubridate
# - you will always be using tidyverse. If you aren't then you're doing something
#   wrong.
library(tidyverse)
library(readxl)
library(rvest)
library(RCurl)
library(lubridate)

## 1. Download data #####################

# 1. Designate the target webpage to scrape for data
#   Paste the url of the target webpage here, wrapped in quotation marks

URL <- ""

# 2. Name the files
#   Add the names for each file into this list, wrapped in quotation marks, IN 
#   THE SAME ORDER THAT THEY ARE LISTED ON THE WEBPAGE ITSELF. Include the file
#   extension (.csv, .xlsx, etc.) in the name of the file as well.

FILE_NAMES <- list(# Name_1, Name_2, Name_3, etc.
)

# 3. Designate file path of where these data files will go in the CCRCN library
#   Paste the file path here, wrapped in quotation marks. The getwd() function 
#   will automatically detect the working directory of the R project (in the case 
#   of the CCRCN Data library, the location of where this repository is stored on 
#   your local drive + "CCRCN-Data-Library"), which will be pasted in combination
#   with whatever you include within the quotation marks.

FILE_PATH <- paste0(getwd(), "", "/" )

# 4. Designate the html node and attri that the web scraper will search for. This
#   will correspond to what classes of div's the data URL(s) are held in. You can
#   find out this information by inspecting the web page (right click + inspect)
#   or using a web browser plug-in (e.g., Google Chrome Web Scraper:
#   https://chrome.google.com/webstore/detail/web-scraper/jnhgnonknehpejjnehehllkliplmbmhn?hl=en)

HTML_NODE <- # E.g., ".sb-download-link"
  
HTML_ATTR <- # E.g., "data-url"

# The stem of the url should always be the same
BASE_URL <- # E.g., "https://www.sciencebase.gov"

print("If you are downloading data, un-comment the following lines")
# page <- read_html(URL)
# 
# # Extract the url paths for each data file embedded on the webpage, and save
# #   those paths to a list
# url_list <- page %>%
#   html_nodes(HTML_NODE) %>% 
#   html_attr(HTML_ATTR)
# 
# # For each data file path on the webpage....
# for (i in 1:length(url_list)) {
#   
#   # ...extract and download file
#   download.file(paste0(BASE_URL, url_list[[i]]), paste0(FILE_PATH, FILE_NAMES[[i]]),
#                 mode = "wb")
# }

## 2. Read in data #####################

# The original data from the table is stored in the Drexler_et_al_2009_peat_accretion_age_depth.csv file. 
# Because the site-core-sample ID field could not easily be separated using tidy logic, I did it manually in excel
# Additionally, the negative sign character used in the elevation field was causing problems in both excel and R and was replaced with (-)
# That edited file is read in 
# Different computers are having trouble parsing the column names with spaces, leading to errors during cutation. 
# Renaming columns in the read_excel call and skipping the first line which represents the old column names

# Designate column types 
# Does the imported data have column names?

# NOTE: no need to import the same data file twice. so you may not necessarily
#   have an impact_raw, species_raw, etc. if all of that data is coming from the
#   same data file (or is being digitized/generated by hand)

# There also may be multiple data files corresponding to a single data level 
#   (e.g. two files, both with depthseries data, which will be joined together)
depthseries_raw <- read_excel("./data/authorName_year/original/data.xls", 
                             sheet = NULL, # If more than one sheet, which one?
                             col_names = TRUE, # Do you want to rename any columns while importing?
                             col_types = NULL, # Encode columns as correct types?
                             # ^ Doing this will entail reading in and seeing
                             #  what the defaults are
                             skip = 0 # Do you need to skip any lines?
                             )

cores_raw <- read_csv("./data/authorName_year/original/core_data.csv")

impact_raw <- read_csv("./data/authorName_year/original/impact_data.csv")

species_raw <- read_csv("./data/authorName_year/original/species_data.csv")

# Few datasets will include biomass, at the time of writing at least
biomass_raw <- read_csv("./data/authorName_year/original/biomass_data.csv")

## 3. Curate data ######################
# If there is any preliminary prep needed to do, write such code here
## ....3A. Depthseries data ##################

# Use curation functions if you need
source("./scripts/1_data_formatting/curation_functions.R")

depthseries <- depthseries_raw   

## ....3B. Core-level data ##################

# Use curation functions if you need
source("./scripts/1_data_formatting/curation_functions.R")

cores <- cores_raw

## ....3C. Site-level data #############

## ....3D. Impact data ###################
impacts <- impact_raw

## ....3E. Species data #############

species <- species_raw
## ....3F. Matierals and Methods #############

methods <- methods_raw

## ....3G. Study-level data #########

# import the CCRCN bibliography 
CCRCN_bib <- bib2df("./docs/CCRCN_bibliography.bib")

# link each study to primary citation and join with synthesis table
study_IDs <- unique(cores$study_id)

studies <- CCRCN_bib %>%
  select(BIBTEXKEY, CATEGORY, DOI) %>%
  rename(bibliography_id = BIBTEXKEY,
         study_type = CATEGORY,
         doi = DOI) %>%
  filter(bibliography_id %in% study_IDs) %>%
  mutate(study_id = bibliography_id) %>%
  mutate(study_type = tolower(study_type)) %>%
  select(study_id, study_type, bibliography_id, doi) 

# Further addition of attributes to study-level data
studies <- studies

## 4. QA/QC of data ################
source("./scripts/1_data_formatting/qa_functions.R")

## ....4A. Column and Variable names ###############
# Make sure column names are formatted correctly
test_colnames("core_level", cores)
test_colnames("depthseries", depthseries)
test_colnames("site_level", sites)
test_colnames("impact", impacts)
test_colnames("species", species)
test_colnames("study_information", studies)

# Make sure variable names are fomatted correctly
test_varnames(cores)
test_varnames(depthseries)
test_varnames(sites)
test_varnames(impacts)
test_varnames(species)
test_varnames(studies)

# Select only controlled attributes, and re-order them according to CCRCN guidance
select_and_reorder_columns(cores)
select_and_reorder_columns(depthseries)
select_and_reorder_columns(sites)
select_and_reorder_columns(impacts)
select_and_reorder_columns(species)
select_and_reorder_columns(studies)

## ....4B. Quality control on cell values ###################
# Make sure that all core IDs are unique
test_unique_cores(cores)

# Provide summary stats on each numeric column, to check for oddities
numeric_test_results <- test_numeric_vars(depthseries)

# Test relationships between core_ids at core- and depthseries-levels
# the test returns all core-level rows that did not have a match in the depth series data
results <- test_core_relationships(cores_updated, depthseries_joined)

## 5. Write data ######################
write_csv(depthseries, "./data/authorName_year/derivative/authorName_et_al_year_depthseries.csv")
write_csv(cores, "./data/authorName_year/derivative/authorName_et_al_year_cores.csv")
write_csv(sites, "./data/authorName_year/derivative/authorName_et_al_year_sites.csv")
write_csv(impacts, "./data/authorName_year/derivative/authorName_et_al_year_impacts.csv")
write_csv(species, "./data/authorName_year/derivative/authorName_et_al_year_species.csv")
write_csv(studies, "./data/authorName_year/derivative/authorName_et_al_year_study_citations.csv")

