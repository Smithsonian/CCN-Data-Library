# Coastal Carbon Research Coordination Network
# Hook for Krauss 2018, jointly paired with Jones 2017
# Contact: klingesd@si.edu
#          
## Citations
# Krauss, K.W., Noe, G.B., Duberstein, J.A., Conner, W.H., Stagg, C.L., Cormier, 
# N., Jones, M.C., Bernhardt, C.J., Lockaby, B.G., From, A.S., Doyle, T.W., Day, 
# R.H., Ensign, S.H., Pierfelice, K.N., Hupp, C.R., Chow, A.T., and Whitbeck, J.L., 
# 2018, The role of the upper tidal estuary in wetland blue carbon storage and 
# flux: Global Biogeochemical Cycles, v. 32, no. 5, p. 817-839, 
# https://doi.org/10.1029/2018GB005897.

# Krauss 2018 data release:
# 10.5066/F7TM7930

## 0. Workspace prep ###################

# NOTE: 

# Not all of these packages will be helpful for every hook:
# - if you don't download the data, you can remove the rvest and RCurl
# - if you don't need to import excel spreadsheets to R (as in, your data is in
#   .csv or tab-delimited formats). you can remove readxl
# - if you won't be working with date or time series data, you can remove lubridate
# - you will always be using tidyverse. If you aren't then you're doing something
#   wrong.
library(tidyverse)
library(readxl)
library(rvest)
library(RCurl)
library(lubridate)

## 1. Download data #####################

# 1. Designate the target webpage to scrape for data
#   Paste the url of the target webpage here, wrapped in quotation marks

URL <- "https://www.sciencebase.gov/catalog/item/59e9f79de4b05fe04cd690a1"

# 2. Name the files
#   Add the names for each file into this list, wrapped in quotation marks, IN 
#   THE SAME ORDER THAT THEY ARE LISTED ON THE WEBPAGE ITSELF. Include the file
#   extension (.csv, .xlsx, etc.) in the name of the file as well.

FILE_NAMES <- list("TFFW_Carbon_Budget_GHG.csv", "TFFW_Carbon_Budget_Root_Ingrowth.csv",
                   "TFFW_Carbon_Budget_Stand_Structure_2005_and_2012.csv", 
                   "TFFW_Carbon_Budget_Wood_Increment_and Litterfall_2005_2015.csv",
                   "TFFW_Carbon_Budget_WoodyDebris.csv",
                   "TFFW_elemental_carbon.csv",
                   "TFFW_Radiocarbon.csv",
                   "TFFW_soil_core_data.csv",
                   "TFFW_Carbon_metadata.xml"
)

# 3. Designate file path of where these data files will go in the CCRCN library
#   Paste the file path here, wrapped in quotation marks. The getwd() function 
#   will automatically detect the working directory of the R project (in the case 
#   of the CCRCN Data library, the location of where this repository is stored on 
#   your local drive + "CCRCN-Data-Library"), which will be pasted in combination
#   with whatever you include within the quotation marks.

FILE_PATH <- paste0(getwd(), "/data/Krauss_2018/original", "/" )

# 4. Designate the html node and attri that the web scraper will search for. This
#   will correspond to what classes of div's the data URL(s) are held in. You can
#   find out this information by inspecting the web page (right click + inspect)
#   or using a web browser plug-in (e.g., Google Chrome Web Scraper:
#   https://chrome.google.com/webstore/detail/web-scraper/jnhgnonknehpejjnehehllkliplmbmhn?hl=en)

HTML_NODE <- ".sb-download-link"
  
  HTML_ATTR <- "data-url"
  
  # The stem of the url should always be the same
  BASE_URL <- "https://www.sciencebase.gov"

print("If you are downloading data, un-comment the following lines")
page <- read_html(URL)

# Extract the url paths for each data file embedded on the webpage, and save
#   those paths to a list
url_list <- page %>%
  html_nodes(HTML_NODE) %>%
  html_attr(HTML_ATTR)

# For each data file path on the webpage....
for (i in 1:length(url_list)) {

  # ...extract and download file
  download.file(paste0(BASE_URL, url_list[[i]]), paste0(FILE_PATH, FILE_NAMES[[i]]),
                mode = "wb")
}

## 2. Read in data #####################

# The original data from the table is stored in the Drexler_et_al_2009_peat_accretion_age_depth.csv file. 
# Because the site-core-sample ID field could not easily be separated using tidy logic, I did it manually in excel
# Additionally, the negative sign character used in the elevation field was causing problems in both excel and R and was replaced with (-)
# That edited file is read in 
# Different computers are having trouble parsing the column names with spaces, leading to errors during cutation. 
# Renaming columns in the read_excel call and skipping the first line which represents the old column names

# Designate column types 
# Does the imported data have column names?

# NOTE: no need to import the same data file twice. so you may not necessarily
#   have an impact_raw, species_raw, etc. if all of that data is coming from the
#   same data file (or is being digitized/generated by hand)

# There also may be multiple data files corresponding to a single data level 
#   (e.g. two files, both with depthseries data, which will be joined together)
depthseries_DBD_raw <- read_csv("./data/Krauss_2018/original/TFFW_soil_core_data.csv", 
                              col_names = TRUE, 
                              col_types = cols(
                                "Compression (%)" = col_double()
                              ) 
                            )

depthseries_carbon_raw <- read_csv("./data/Krauss_2018/original/TFFW_elemental_carbon.csv")

radiocarbon_raw <- read_csv("./data/Krauss_2018/original/TFFW_Radiocarbon.csv")

# Few datasets will include biomass, at the time of writing at least
root_productivity <- read_csv("data/Krauss_2018/original/TFFW_Carbon_Budget_Root_Ingrowth.csv")

## 3. Curate data ######################
# If there is any preliminary prep needed to do, write such code here
## ....3A. Depthseries data ##################

## ......3Aa. Prep depthseries from Krauss #####################
# Prep radiocarbon data to join to DBD data
radiocarbon <- radiocarbon_raw %>%
  separate("Depth (cm)", into = c("depth_min", "depth_max"), sep = "-") %>%
  mutate(depth_max = as.double(depth_max), depth_min = as.double(depth_min))

Krauss_depthseries <- depthseries_DBD_raw %>%
  rename(depth_max = "Depth (cm)") %>% # See readme for assumptions on depth
  mutate(depth_min = depth_max - 1) %>% # See readme for assumptions on depth

  # Now join in the radioncarbon data before we rename anything
  full_join(radiocarbon,  by = c("River", "Core ID", "depth_min", "depth_max")) %>%

  # Ok, now good to rename
  rename(core_id = "Core ID", dry_bulk_density = "Dry bulk density (g/cc)",
         "fraction_organic_matter" = "LOI (%)", site_id = River, 
         c14_material = "Material Dated", compaction_fraction = "Compression (%)",
         sample_id = "Lab ID", c14_age = "14C Age", c14_age_sd = "SE") %>% 
  
  # Recode cores
  mutate(core_id = recode(core_id, "11-11-2-1" = "turkey_creek_oligohaline",
                          "11-11-2-3" = "turkey_creek_salty_impacted",
                          "11-11-3-1" = "butler_island_fresh_tidal",
                          "11-11-1-2" = "richmond_island_upper_fresh",
                          "12-12-10-3" = "savannah_oligohaline",
                          "12-12-10-2" = "savannah_salty_impacted",
                          "12-12-9-3" = "savannah_fresh_tidal",
                          "12-12-11-1" = "savannah_upper_fresh"
  )) %>%
  # Add 'River' to site names
  mutate(site_id = paste(site_id, "River", sep = "_")) %>%
  # manually add core coordinates sent to JH, ML, and DK on 2019-04-19
  # email subject: FW: [EXTERNAL] Positional Information for Krauss and Jones Data Release
  mutate(core_latitude = ifelse(core_id == "turkey_creek_oligohaline", 33.35003,
                          ifelse(core_id == "turkey_creek_salty_impacted", 33.34001,
                          ifelse(core_id == "butler_island_fresh_tidal", 33.422823,
                            ifelse(core_id == "richmond_island_upper_fresh", 33.55564,
                              ifelse(core_id == "savannah_oligohaline", 32.17,
                                ifelse(core_id == "savannah_salty_impacted", 32.18, 
                                 ifelse(core_id == "savannah_fresh_tidal", 32.24, 
                                  ifelse(core_id == "savannah_fresh_tidal", 32.238, 
                                   NA))))))))) %>%
  mutate(core_longitude = ifelse(core_id == "turkey_creek_oligohaline", -79.3447,
                           ifelse(core_id == "turkey_creek_salty_impacted", -79.34166,
                            ifelse(core_id == "butler_island_fresh_tidal", -79.207996,
                             ifelse(core_id == "richmond_island_upper_fresh", -79.08943,
                               ifelse(core_id == "savannah_oligohaline", -81.14,
                                 ifelse(core_id == "savannah_salty_impacted", -81.14, 
                                  ifelse(core_id == "savannah_fresh_tidal", -81.15, 
                                    ifelse(core_id == "savannah_fresh_tidal", -81.155, 
                                       NA)))))))))
  

## ......3Ab. COMMENTED OUT Prep depthseries from Jones  #####################

# # only including if resolved to join in c14 ages from Jones 2017
# geochron_oligo <- geochron_oligo %>%
#   mutate(core_id = "Oligohaline_Marsh")
# 
# geochron_heavy_salt <- geochron_heavy_salt %>%
#   mutate(core_id = "Heavily_Salt_Impacted_Swamp")
# 
# geochron_mod_salt <- geochron_mod_salt %>%
#   mutate(core_id = "Moderately_Salt_Impacted")
# 
# source("./scripts/1_data_formatting/curation_functions.R")  
# geochron <- geochron_oligo %>%
#   bind_rows(geochron_heavy_salt) %>%
#   bind_rows(geochron_mod_salt) %>%
#   mutate(study_id = "Jones_et_al_2017") %>%
#   mutate(depth_min = Depth - (Thickness/2)) %>%
#   mutate(depth_max = Depth + (Thickness/2)) %>%
#   rename(sample_id = SampleID, c14_age = Age, c14_material = MaterialDated,
#          c14_age_sd = ErrorOlder) %>%
#   mutate(c14_age = ifelse(c14_age == 0, NA, c14_age)) %>%
#   mutate(c14_notes = ifelse(is.na(c14_age), "c14 age yielded greater than modern day", NA)) %>%
#   mutate(sample_id = as.character(sample_id)) %>%
#   mutate(c14_age = ifelse(c14_age == 0, NA, c14_age)) %>%
#   mutate(c14_notes = ifelse(is.na(c14_age), "c14 age yielded greater than modern day", NA)) %>%
#   select(study_id, core_id, sample_id, depth_min, depth_max, c14_age, c14_age_sd, 
#          c14_notes, c14_material)
# 
# # Coerce into matrix and transpose
# LOI_oligo <- t(as.matrix(LOI_oligo_raw))
# LOI_oligo <- as.data.frame(LOI_oligo)
# LOI_oligo <- as_tibble(LOI_oligo, rownames = NULL) %>%
#   slice(-1:-5) %>%
#   mutate(core_id = "Oligohaline_Marsh")
# 
# # Coerce into matrix and transpose
# LOI_heavy_salt <- t(as.matrix(LOI_heavy_salt_raw))
# LOI_heavy_salt <- as.data.frame(LOI_heavy_salt)
# LOI_heavy_salt <- as_tibble(LOI_heavy_salt, rownames = NULL) %>%
#   slice(-1:-5) %>%
#   mutate(core_id = "Heavily_Salt_Impacted_Swamp")
# 
# 
# # Coerce into matrix and transpose
# LOI_mod_salt <- t(as.matrix(LOI_mod_salt_raw))
# LOI_mod_salt <- as.data.frame(LOI_mod_salt)
# LOI_mod_salt <- as_tibble(LOI_mod_salt, rownames = NULL) %>%
#   slice(-1:-5) %>%
#   mutate(core_id = "Moderately_Salt_Impacted")
# 
# # Join LOI datasets
# LOI <- LOI_oligo %>%
#   bind_rows(LOI_heavy_salt) %>%
#   bind_rows(LOI_mod_salt) %>%
#   separate(`V1`, into = c("depth_min", "depth_max"), sep = "-") %>%
#   mutate(depth_min = as.double(depth_min)) %>%
#   mutate(depth_max = as.double(gsub(" cm", "", depth_max))) %>%
#   separate("V6", into = c("age_max", "age", "age_min"), sep = "/") %>%
#   mutate(age_max = as.double(age_max), age = as.double(age),
#          age_min = as.double(age_min), 
#          age_depth_model_reference = "YBP") %>%
#   rename(sample_id = "V5")
# 
# Jones_depthseries <- geochron %>%
#   full_join(LOI, by = c("core_id", "sample_id", "depth_min", "depth_max")) %>%
#   arrange(core_id, depth_min) %>%
#   select(core_id, sample_id, depth_min, depth_max, c14_notes,
#          age, age_min, age_max, age_depth_model_reference)
  

## ......3Ac. COMMENTED OUT Join Krauss and Jones depthseries data  #####################

# only including if resolved to join in c14 ages from Jones 2017
# depthseries <- Krauss_depthseries %>%
#   full_join(Jones_depthseries)

## ....3B. Core-level data ##################

# Use curation functions if you need
source("./scripts/1_data_formatting/curation_functions.R")

cores <- depthseries_DBD_raw %>%
  # Pull core IDs given to Jones 2017
  rename(core_id = "Core ID", site_id = River) %>%



## ....3C. Site-level data #############

## ....3D. Impact data #################
impacts <- impact_raw

## ....3E. Species data #############

species <- species_raw

## ....3F. Materials and Methods #############

methods <- cores %>%
  select(core_id) %>%
  distinct(core_id) %>%
  mutate(loss_on_ignition_temperature = c(550)) %>%
  # see personal communication, different coring methods for each site
  mutate(coring_method = ifelse(site_id = "Waccamaw_River", "vibracore",
                                "russian corer"))  %>%
  mutate(compaction_flag = ifelse(site_id = "Waccamaw_River", "compaction quantified",
                                  "corer limits compaction"))
  

## ....3G. Study-level data #########


## 4. QA/QC of data ################
source("./scripts/1_data_formatting/qa_functions.R")

## ....4A. Column and Variable names ###############
# Make sure column names are formatted correctly
test_colnames("core_level", cores)
test_colnames("depthseries", depthseries)
test_colnames("site_level", sites)
test_colnames("impact", impacts)
test_colnames("species", species)
test_colnames("study_information", studies)

# Make sure variable names are fomatted correctly
test_varnames(cores)
test_varnames(depthseries)
test_varnames(sites)
test_varnames(impacts)
test_varnames(species)
test_varnames(studies)

# Select only controlled attributes, and re-order them according to CCRCN guidance
select_and_reorder_columns(cores)
select_and_reorder_columns(depthseries)
select_and_reorder_columns(sites)
select_and_reorder_columns(impacts)
select_and_reorder_columns(species)
select_and_reorder_columns(studies)

## ....4B. Quality control on cell values ###################
# Make sure that all core IDs are unique
test_unique_cores(cores)

# Provide summary stats on each numeric column, to check for oddities
numeric_test_results <- test_numeric_vars(depthseries)

# Test relationships between core_ids at core- and depthseries-levels
# the test returns all core-level rows that did not have a match in the depth series data
results <- test_core_relationships(cores_updated, depthseries_joined)

## 5. Write data ######################
write_csv(depthseries, "./data/authorName_year/derivative/authorName_et_al_year_depthseries.csv")
write_csv(cores, "./data/authorName_year/derivative/authorName_et_al_year_cores.csv")
write_csv(sites, "./data/authorName_year/derivative/authorName_et_al_year_sites.csv")
write_csv(impacts, "./data/authorName_year/derivative/authorName_et_al_year_impacts.csv")
write_csv(species, "./data/authorName_year/derivative/authorName_et_al_year_species.csv")
write_csv(studies, "./data/authorName_year/derivative/authorName_et_al_year_study_citations.csv")

